python train_mol_unified.py --model-type somt --save-dir ./checkpoints/somt_mol_run_0 --seed 2025 --epochs 3 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv
ðŸš€ Using cuda | Model=somt | seed=2025
âœ… Special tokens bound: 0 1 2 3 4
ðŸ“š Dataset: 24147 samples â†’ Train: 21732, Test: 2415 | Vocab: 793
ðŸ§  Model built with 3.77M params
Epoch 1: train 1.178 | eval 0.279 | ppl 1.32
Epoch 2: train 0.292 | eval 0.429 | ppl 1.54
Epoch 3: train 0.502 | eval 0.437 | ppl 1.55
âœ… Metrics saved â†’ ./checkpoints/somt_mol_run_0/metrics.json

(base) D:\Work\SCMT>python comparison_mol.py --runs 3 --extra-args "--epochs 3 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv"

ðŸš€ Running SOMT (run 0, seed=2025)
CMD: python train_mol_unified.py --model-type somt --save-dir ./checkpoints/somt_mol_run_0 --seed 2025 --epochs 3 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv
ðŸš€ Using cuda | Model=somt | seed=2025
âœ… Special tokens bound: 0 1 2 3 4
ðŸ“š Dataset: 24147 samples â†’ Train: 21732, Test: 2415 | Vocab: 793
ðŸ§  Model built with 3.77M params
Epoch 1: train 1.178 | eval 0.279 | ppl 1.32
Epoch 2: train 0.292 | eval 0.429 | ppl 1.54
Epoch 3: train 0.502 | eval 0.437 | ppl 1.55
âœ… Metrics saved â†’ ./checkpoints/somt_mol_run_0/metrics.json
   ðŸ§© Split (SOMT run 0): {'train_size': 21732, 'test_size': 2415, 'seed': 2025}

ðŸš€ Running SOMT (run 1, seed=2026)
CMD: python train_mol_unified.py --model-type somt --save-dir ./checkpoints/somt_mol_run_1 --seed 2026 --epochs 3 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv
ðŸš€ Using cuda | Model=somt | seed=2026
âœ… Special tokens bound: 0 1 2 3 4
ðŸ“š Dataset: 24147 samples â†’ Train: 21732, Test: 2415 | Vocab: 793
ðŸ§  Model built with 3.77M params
Epoch 1: train 1.950 | eval 1.499 | ppl 4.48
Epoch 2: train 1.290 | eval 1.093 | ppl 2.98
Epoch 3: train 1.096 | eval 1.001 | ppl 2.72
âœ… Metrics saved â†’ ./checkpoints/somt_mol_run_1/metrics.json
   ðŸ§© Split (SOMT run 1): {'train_size': 21732, 'test_size': 2415, 'seed': 2026}

ðŸš€ Running SOMT (run 2, seed=2027)
CMD: python train_mol_unified.py --model-type somt --save-dir ./checkpoints/somt_mol_run_2 --seed 2027 --epochs 3 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv
ðŸš€ Using cuda | Model=somt | seed=2027
âœ… Special tokens bound: 0 1 2 3 4
ðŸ“š Dataset: 24147 samples â†’ Train: 21732, Test: 2415 | Vocab: 793
ðŸ§  Model built with 3.77M params
Epoch 1: train 1.126 | eval 0.106 | ppl 1.11
Epoch 2: train 0.283 | eval 0.182 | ppl 1.20
Epoch 3: train 0.280 | eval 0.347 | ppl 1.42
âœ… Metrics saved â†’ ./checkpoints/somt_mol_run_2/metrics.json
   ðŸ§© Split (SOMT run 2): {'train_size': 21732, 'test_size': 2415, 'seed': 2027}

ðŸš€ Running GPT2 (run 0, seed=2025)
CMD: python train_mol_unified.py --model-type gpt2 --save-dir ./checkpoints/gpt2_mol_run_0 --seed 2025 --epochs 3 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv
ðŸš€ Using cuda | Model=gpt2 | seed=2025
âœ… Special tokens bound: 0 1 2 3 4
ðŸ“š Dataset: 24147 samples â†’ Train: 21732, Test: 2415 | Vocab: 793
ðŸ§  Model built with 3.39M params
Epoch 1/3:   0%|                                                                              | 0/2717 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
Epoch 1: train 0.745 | eval 5.110 | ppl 165.62
Epoch 2: train 0.592 | eval 5.676 | ppl 291.77
Epoch 3: train 0.527 | eval 5.883 | ppl 358.99
âœ… Metrics saved â†’ ./checkpoints/gpt2_mol_run_0/metrics.json
   ðŸ§© Split (GPT2 run 0): {'train_size': 21732, 'test_size': 2415, 'seed': 2025}

ðŸš€ Running GPT2 (run 1, seed=2026)
CMD: python train_mol_unified.py --model-type gpt2 --save-dir ./checkpoints/gpt2_mol_run_1 --seed 2026 --epochs 3 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv
ðŸš€ Using cuda | Model=gpt2 | seed=2026
âœ… Special tokens bound: 0 1 2 3 4
ðŸ“š Dataset: 24147 samples â†’ Train: 21732, Test: 2415 | Vocab: 793
ðŸ§  Model built with 3.39M params
Epoch 1/3:   0%|                                                                              | 0/2717 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
Epoch 1: train 0.740 | eval 5.087 | ppl 161.98
Epoch 2: train 0.586 | eval 5.550 | ppl 257.35
Epoch 3: train 0.522 | eval 5.968 | ppl 390.59
âœ… Metrics saved â†’ ./checkpoints/gpt2_mol_run_1/metrics.json
   ðŸ§© Split (GPT2 run 1): {'train_size': 21732, 'test_size': 2415, 'seed': 2026}

ðŸš€ Running GPT2 (run 2, seed=2027)
CMD: python train_mol_unified.py --model-type gpt2 --save-dir ./checkpoints/gpt2_mol_run_2 --seed 2027 --epochs 3 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv
ðŸš€ Using cuda | Model=gpt2 | seed=2027
âœ… Special tokens bound: 0 1 2 3 4
ðŸ“š Dataset: 24147 samples â†’ Train: 21732, Test: 2415 | Vocab: 793
ðŸ§  Model built with 3.39M params
Epoch 1/3:   0%|                                                                              | 0/2717 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
Epoch 1: train 0.741 | eval 5.155 | ppl 173.30
Epoch 2: train 0.586 | eval 5.561 | ppl 260.09
Epoch 3: train 0.522 | eval 5.915 | ppl 370.56
âœ… Metrics saved â†’ ./checkpoints/gpt2_mol_run_2/metrics.json
   ðŸ§© Split (GPT2 run 2): {'train_size': 21732, 'test_size': 2415, 'seed': 2027}

âœ… Results saved to mol_ab_results.json
ðŸ“Š Parameter Counts:
  somt: 3.99M params

ðŸ“ˆ Summary (mean eval loss & PPL):
  somt: mean eval loss = 0.5970 | mean ppl = 1.82 Â± 0.93
  gpt2: mean eval loss = 5.5451 | mean ppl = 255.97 Â± 2.23

ðŸŽ¯ Done. Each run used deterministic seeds offset by run index.