python comparison_mol.py --runs 3 --extra-args "--epochs 1 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv"

ðŸš€ Running SOMT (run 0, seed=2025)
CMD: python train_mol_unified.py --model-type somt --save-dir ./checkpoints/somt_mol_run_0 --seed 2025 --epochs 1 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv
ðŸš€ Using cuda | Model=somt | seed=2025
âœ… Special tokens bound: 0 1 2 3 4
ðŸ“š Dataset: 24147 samples â†’ Train: 21732, Test: 2415 | Vocab: 793
ðŸ§  Model built with 3.77M params
Epoch 1: train 1.178 | eval 0.279 | ppl 1.32
âœ… Metrics saved â†’ ./checkpoints/somt_mol_run_0/metrics.json
   ðŸ§© Split (SOMT run 0): {'train_size': 21732, 'test_size': 2415, 'seed': 2025}

ðŸš€ Running SOMT (run 1, seed=2026)
CMD: python train_mol_unified.py --model-type somt --save-dir ./checkpoints/somt_mol_run_1 --seed 2026 --epochs 1 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv
ðŸš€ Using cuda | Model=somt | seed=2026
âœ… Special tokens bound: 0 1 2 3 4
ðŸ“š Dataset: 24147 samples â†’ Train: 21732, Test: 2415 | Vocab: 793
ðŸ§  Model built with 3.77M params
Epoch 1: train 1.950 | eval 1.499 | ppl 4.48
âœ… Metrics saved â†’ ./checkpoints/somt_mol_run_1/metrics.json
   ðŸ§© Split (SOMT run 1): {'train_size': 21732, 'test_size': 2415, 'seed': 2026}

ðŸš€ Running SOMT (run 2, seed=2027)
CMD: python train_mol_unified.py --model-type somt --save-dir ./checkpoints/somt_mol_run_2 --seed 2027 --epochs 1 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv
ðŸš€ Using cuda | Model=somt | seed=2027
âœ… Special tokens bound: 0 1 2 3 4
ðŸ“š Dataset: 24147 samples â†’ Train: 21732, Test: 2415 | Vocab: 793
ðŸ§  Model built with 3.77M params
Epoch 1: train 1.126 | eval 0.106 | ppl 1.11
âœ… Metrics saved â†’ ./checkpoints/somt_mol_run_2/metrics.json
   ðŸ§© Split (SOMT run 2): {'train_size': 21732, 'test_size': 2415, 'seed': 2027}

ðŸš€ Running GPT2 (run 0, seed=2025)
CMD: python train_mol_unified.py --model-type gpt2 --save-dir ./checkpoints/gpt2_mol_run_0 --seed 2025 --epochs 1 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv
ðŸš€ Using cuda | Model=gpt2 | seed=2025
âœ… Special tokens bound: 0 1 2 3 4
ðŸ“š Dataset: 24147 samples â†’ Train: 21732, Test: 2415 | Vocab: 793
ðŸ§  Model built with 3.39M params
Epoch 1/1:   0%|                                                                              | 0/2717 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
You may ignore this warning if your `pad_token_id` (0) is identical to the `bos_token_id` (0), `eos_token_id` (1), or the `sep_token_id` (None), and your input is not padded.
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
Epoch 1: train 0.745 | eval 5.110 | ppl 165.62
âœ… Metrics saved â†’ ./checkpoints/gpt2_mol_run_0/metrics.json
   ðŸ§© Split (GPT2 run 0): {'train_size': 21732, 'test_size': 2415, 'seed': 2025}

ðŸš€ Running GPT2 (run 1, seed=2026)
CMD: python train_mol_unified.py --model-type gpt2 --save-dir ./checkpoints/gpt2_mol_run_1 --seed 2026 --epochs 1 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv
ðŸš€ Using cuda | Model=gpt2 | seed=2026
âœ… Special tokens bound: 0 1 2 3 4
ðŸ“š Dataset: 24147 samples â†’ Train: 21732, Test: 2415 | Vocab: 793
ðŸ§  Model built with 3.39M params
Epoch 1/1:   0%|                                                                              | 0/2717 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
You may ignore this warning if your `pad_token_id` (0) is identical to the `bos_token_id` (0), `eos_token_id` (1), or the `sep_token_id` (None), and your input is not padded.
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
Epoch 1: train 0.740 | eval 5.087 | ppl 161.98
âœ… Metrics saved â†’ ./checkpoints/gpt2_mol_run_1/metrics.json
   ðŸ§© Split (GPT2 run 1): {'train_size': 21732, 'test_size': 2415, 'seed': 2026}

ðŸš€ Running GPT2 (run 2, seed=2027)
CMD: python train_mol_unified.py --model-type gpt2 --save-dir ./checkpoints/gpt2_mol_run_2 --seed 2027 --epochs 1 --seq-len 90 --batch-size 8 --lr 2e-4 --data ./data/test.csv
ðŸš€ Using cuda | Model=gpt2 | seed=2027
âœ… Special tokens bound: 0 1 2 3 4
ðŸ“š Dataset: 24147 samples â†’ Train: 21732, Test: 2415 | Vocab: 793
ðŸ§  Model built with 3.39M params
Epoch 1/1:   0%|                                                                              | 0/2717 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
You may ignore this warning if your `pad_token_id` (0) is identical to the `bos_token_id` (0), `eos_token_id` (1), or the `sep_token_id` (None), and your input is not padded.
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
Epoch 1: train 0.741 | eval 5.155 | ppl 173.30
âœ… Metrics saved â†’ ./checkpoints/gpt2_mol_run_2/metrics.json
   ðŸ§© Split (GPT2 run 2): {'train_size': 21732, 'test_size': 2415, 'seed': 2027}

âœ… Results saved to mol_ab_results.json
ðŸ“Š Parameter Counts:
  somt: 3.99M params

ðŸ“ˆ Summary (mean eval loss & PPL):
  somt: mean eval loss = 0.6277 | mean ppl = 1.87 Â± 1.54
  gpt2: mean eval loss = 5.1174 | mean ppl = 166.90 Â± 4.72

ðŸŽ¯ Done. Each run used deterministic seeds offset by run index.